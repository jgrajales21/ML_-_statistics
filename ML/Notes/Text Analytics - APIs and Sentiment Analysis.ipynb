{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1947ae83",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Text Analytics</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c9794",
   "metadata": {},
   "source": [
    "<li> <span style=\"color:red\">Text Analytics</span> is the process of using a machine to extract <span style=\"color:green\">relevant</span> information from text</li>\n",
    "<li>common packages:</li>\n",
    "<ul>\n",
    "    <li>nltk: python's native natural language toolkit. <a href=\"https://www.nltk.org/data.html\">documentation</a></li>\n",
    "    <li>stanford corenlp: Java based package for text analytics with APIs in numerous languages (including python) <a href=\"https://stanfordnlp.github.io/CoreNLP/\">documentation</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726de44",
   "metadata": {},
   "source": [
    "<li>install nltk using pip <span style=\"color:blue\">!pip install nltk --upgrade</span></li>\n",
    "<li>My version: 3.8.1</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c248c9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ade1a0",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">nltk data</span>\n",
    "<li>install corpora, grammar, pre-trained models, etc. (see next cell)</li>\n",
    "<li>This is a data download - you need to do this only once!</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b5d3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTANT: nltk.download() install all, and then close the window to continue using your notebook\n",
    "#All files will be downloaded to a folder nltk_data in your home directory\n",
    "\n",
    "#The download window will probably come up behind your browser, so look for it\n",
    "\n",
    "#You will need to close it to be able to use your notebook again\n",
    "\n",
    "#This downloads all the stuff you need so you YOU NEED TO DO THIS ONLY ONCE!!!\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2c447",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">ntlk resources</span> \n",
    "<p></p>\n",
    "<li><a href=\"https://www.nltk.org/\">https://www.nltk.org/</a></li>\n",
    "<li><a href=\"https://www.nltk.org/book/\">The nltk book</a></li>\n",
    "<li><a href=\"https://cheatography.com/murenei/cheat-sheets/natural-language-processing-with-python-and-nltk/\">quick reference</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c80c0e",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:40px\">Types of analysis</span>\n",
    "<br><br>\n",
    "<li>Sentiment analysis: Deciding whether a document (or concept) is positive or negative\n",
    "<li>Entity analysis: Identifying entities (Named entities, Parts of speech) and properties of these entities\n",
    "<li>Topic analysis: Identifying the major topics associated with a piece of text\n",
    "<li>Text summarization: Summarizing a document (Cliff notes version!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13baa817",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Sentiment Analysis</span>\n",
    "<p></p>\n",
    "<span style=\"color:blue;font-size:large\">Basic idea:</span>\n",
    "\n",
    "<li>Identify entities and emotions in a sentence and use these to determine if the entity is being viewed positively or negatively</li>\n",
    "\n",
    "<p></p>\n",
    "<span style=\"color:blue;font-size:large\">Easy examples</span>\n",
    "<li>I had an <b style=\"color:green\">excellent</b> souffle at the restaurant Cavity Maker</li>\n",
    "<li>Excellent is a positive word for both the souffle as well as for the restaurant</li>\n",
    "<p></p>\n",
    "<span style=\"color:blue;font-size:large\">Not so easy examples</span>\n",
    "<li>Often, looking at words alone is not enough to figure out the sentiment</li>\n",
    "<ul>\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for a ‘stuck at home’ snow day</i></li> This one is easy since it includes an explicit positive opinion using a positive word<p>\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for using as a liner for your cat’s litter box</i></li> Not so simple! The positive word \"excellent\" is used with a negative connotation. <p>\n",
    "<li><i>The Girl on the Train is <span style=\"color:green\">better</span> than Gone Girl</i></li> The positive word is used as a comparator. Whether the writer likes The Girl on the Train or not depends on what he or she thinks of Gone Girl\n",
    "    </ul>\n",
    "\n",
    "<p></p>\n",
    "<span style=\"color:blue;font-size:large\">Bottom line</span>\n",
    "<li>Sentiment analysis is generally a starting point in analyzing a text and is then coupled with other techniques (e.g., topic analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955d9f3",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Basic sentiment analysis</span>\n",
    "<p></p>\n",
    "<li>Usually done using a corpus of positive and negative words</li>\n",
    "<li>Some sources compile lists of positive and negative words\n",
    "<li>Others include the polarity - the degree of positivity or negativity - of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd5c98",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Sources of sentiment coded words</span>\n",
    "<p></p>\n",
    "<ol>\n",
    "<li>Hu and Liu's sentiment analysis lexicon: words coded as either positive or negative</li>\n",
    "<ul>\n",
    "<li>http://ptrckprry.com/course/ssd/data/positive-words.txt\n",
    "<li>http://ptrckprry.com/course/ssd/data/negative-words.txt\n",
    "</ul><p>\n",
    "<li>NRC Emotion Lexicon: words coded into emotional categories (many languages)</li>\n",
    "<ul>\n",
    "<li>http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</li>\n",
    "</ul><p>\n",
    "<li>SentiWordNet: Lists of words weighted by positive or negative sentiment. Includes guidance on how to use the words</li>\n",
    "<ul>\n",
    "<li>http://sentiwordnet.isti.cnr.it/</li>\n",
    "</ul><p>\n",
    "<li>Vadar Sentiment tool: 7800 words with positive or negative polarity</li>\n",
    "<ul>\n",
    "<li>Included with python nltk</li>\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81301445",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Polarity based sentiment analysis</span>\n",
    "<p></p>\n",
    "<li><span style=\"color:blue\">VADER (Valence Aware Dictionary for Sentiment Reasoning)</span> is a model based analyzer that assigns polarity to words and also attempts to use the context in which the word is used</li>\n",
    "<li>VADER can distinguish between \"I love you\" (positive); \"I don't love you\" (negative); and \"I don't hate you\" (neutral) because it assigns a polarity to love and hate but uses the don't in a smart way</li>\n",
    "<li>In other words, VADER calculates a sentiment on an entire sentence, not just on individual words</li>\n",
    "<li>VADER also reports a compound score (-1 to +1) that reflects the net positivity or negativity of a sentence</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b701c754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2023.7.22)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b43c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'compound': 0.6369}\n",
      "{'neg': 0.529, 'neu': 0.471, 'pos': 0.0, 'compound': -0.5216}\n",
      "{'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 0.4585}\n",
      "{'neg': 0.649, 'neu': 0.351, 'pos': 0.0, 'compound': -0.5719}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "print(analyzer.polarity_scores(\"I think I love you\")) #Net positive sentence\n",
    "print(analyzer.polarity_scores(\"I don't love you\")) #Negative sentence because of the \"don't\"\n",
    "print(analyzer.polarity_scores(\"I don't hate you\")) #Positive sentence because of the \"don't\"\n",
    "print(analyzer.polarity_scores(\"I hate you\")) #Net negative (but not as negative as the first one was positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f407471",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">API: Application Programming Interface</span>\n",
    "<p></p>\n",
    "<li>An API is a specification (often a defined set of functions) that allow one program to communicate with another program</li>\n",
    "<li>In analytics, APIs are often used to get data from some data resource (e.g., twitter, yelp)</li>\n",
    "<li>Most data providers require that API users obtain authentication keys. This allows the provider to monitor API usage at the user level</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c4a8b",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Yelp API: Sentiment analysis of local restaurants</span>\n",
    "<p></p>\n",
    "<li>Yelp has an API (<a href=\"https://www.yelp.com/developers/documentation/v3\">Yelp Fusion</a>)</li>\n",
    "<li>Each corpus element will consist of a tuple (document id, document text)</li>\n",
    "\n",
    "<span style=\"color:red;\">If you want to do this (but you don't need to do it now)</span>\n",
    "<li>Log into yelp (https://yelp.com). Create an account if you don't have one</li>\n",
    "<li>Go to <a href=\"https://www.yelp.com/developers/documentation/v3\">Yelp Fusion</a></li>\n",
    "<li>Click <span style=\"color:blue\">Manage API Access</span> on the top  menu bar</li>\n",
    "<li>Enter app info (leave optional stuff blank)\n",
    "<li>Copy the client id and API key to a secure place (this notebook should do the trick or use a text file!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24604df",
   "metadata": {},
   "source": [
    "<li>Get reviews of local restaurants from Yelp</li>\n",
    "<ul>\n",
    "    <li>The API URL: https://api.yelp.com/v3/businesses/search returns a list of businesses that match given criteria (location, number, type)</li>\n",
    "    <li>Each returned business has a Yelp business ID</li>\n",
    "    <li>Using this business ID, the API: https://api.yelp.com/v3/businesses/ returns data about the business</li>\n",
    "    <li>Yelp returns a \"review snippet\" consisting of the first sentences of three revies</li>\n",
    "    <li>We'll use these snippets to get the sentiment for each restaurant</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4407594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My API keys \n",
    "with open('/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/yelp_fusion.txt','r') as f:\n",
    "    CLIENT_ID = f.readline().strip()\n",
    "    API_KEY = f.readline().strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c20280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API constants, you shouldn't have to change these.\n",
    "API_HOST = 'https://api.yelp.com' #The API url header\n",
    "SEARCH_PATH = '/v3/businesses/search' #The path for an API request to find businesses\n",
    "BUSINESS_PATH = '/v3/businesses/'  # The path to get data for a single business\n",
    "\n",
    "#This function gets the list of businesses near the location\n",
    "def get_restaurants(api_key,location,number=15):\n",
    "    import requests\n",
    "    \n",
    "     #Set up the search data dictionary. This contains the search parameters\n",
    "    search_data = {\n",
    "    'term': \"restaurant\",\n",
    "    'location': location.replace(' ', '+'),\n",
    "    'limit': number\n",
    "    }\n",
    "    \n",
    "    #Create the API url\n",
    "    url = API_HOST + SEARCH_PATH\n",
    "    \n",
    "    #The API Key data object\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "    \n",
    "    #Get the response object and convert from JSON into a python list\n",
    "    response = requests.request('GET', url, headers=headers, params=search_data).json()\n",
    "    #print(response)\n",
    "    \n",
    "    #Extract the businesses from the response object\n",
    "    businesses = response.get('businesses')\n",
    "    \n",
    "    #Extract the business id and business name for each business into a list\n",
    "    return_data = [(business['id'],business['name']) for business in businesses]\n",
    "    return return_data\n",
    "\n",
    "#This function gets reviews for each business from yelp\n",
    "def get_business_review(api_key,business_id):\n",
    "    import requests\n",
    "    \n",
    "    #API path\n",
    "    business_path = BUSINESS_PATH + business_id+\"/reviews\"\n",
    "    \n",
    "    #API url\n",
    "    url = API_HOST + business_path\n",
    "\n",
    "    #API Key data object\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "\n",
    "    #Get the response and convert from json into a python object\n",
    "    response = requests.request('GET', url, headers=headers).json()\n",
    "    #print(response)\n",
    "    \n",
    "    #Yelp gives review snippets of a few reviews, concatenate these into a single string\n",
    "    review_text = ''\n",
    "    for review in response['reviews']:\n",
    "        review_text += review['text']\n",
    "    return review_text\n",
    "\n",
    "#A function that puts all this together\n",
    "def get_reviews(location,number=15):\n",
    "\n",
    "    #Get the business id and names for restaurants\n",
    "    restaurants = get_restaurants(API_KEY,location,number)\n",
    "\n",
    "    #Take care of the case where there are no restaurants\n",
    "    if not restaurants:\n",
    "        return None\n",
    "    \n",
    "    #Collect reviews by iterating through the restaurants and getting reviews\n",
    "    review_list = list()\n",
    "    for restaurant in restaurants:\n",
    "        restaurant_name = restaurant[1]\n",
    "        restaurant_id = restaurant[0]\n",
    "        review_text = get_business_review(API_KEY,restaurant_id)\n",
    "        \n",
    "        review_list.append((restaurant_name,review_text))\n",
    "    return review_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99611eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_restaurants(API_KEY,\"Columbia University\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = get_business_review(API_KEY,\"h9nuvIu8TyrQcYy8J1AOxg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b71d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reviews(\"Columbia University, New York, NY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_corpus_columbia = get_reviews(\"Columbia University, New York, NY\")\n",
    "yelp_corpus_amherst = get_reviews(\"Amherst College, Amherst, MA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5dcf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_corpus_amherst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d7202",
   "metadata": {},
   "source": [
    "<h4>A function to get the vader sentiment of a document</h4>\n",
    "<li>Get the average sentiment of the sentences in the document</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ff8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment(doc):\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    from nltk import sent_tokenize\n",
    "\n",
    "    #Tokenize into sentences (since vader works best on sentences)\n",
    "    sentences = sent_tokenize(doc)\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    #Initlalize all sentiments to 0\n",
    "    pos=compound=neu=neg=0\n",
    "    for sentence in sentences:\n",
    "        #Get the sentiments\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        pos+=vs['pos']/(len(sentences))\n",
    "        compound+=vs['compound']/(len(sentences))\n",
    "        neu+=vs['neu']/(len(sentences))\n",
    "        neg+=vs['neg']/(len(sentences))\n",
    "    return pos, neg, neu, compound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18eb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment(yelp_corpus_columbia[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75de777",
   "metadata": {},
   "source": [
    "<h4>A function that returns the Vader sentiment of a corpus</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_comparison(corpus):\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    from nltk import sent_tokenize\n",
    "    import pandas as pd\n",
    "    headers = ['doc_id','pos','neg','neu','compound']\n",
    "    df = pd.DataFrame(columns=headers)\n",
    "    df.set_index('doc_id',inplace=True)\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for doc in corpus:\n",
    "        doc_id = doc[0]\n",
    "        pos,neg,neu,compound = vader_sentiment(doc[1])\n",
    "        df.loc[doc_id] = [pos,neg,neu,compound]\n",
    "    return df.sort_values(by=\"compound\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eab98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_comparison(yelp_corpus_columbia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_comparison(yelp_corpus_amherst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc36fb",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-size:40px\">Named entity based sentiment analysis</span>\n",
    "<p></p><li>A <span style=\"color:blue\">named entity</span> is a real world object that can be denoted with a proper name</li>\n",
    "<li>Named entities may be people (e.g., Joe Biden, Bob Dylan), geographical locations (The United States of America, Taj Mahal), organizations (Columbia University, World Bank, Oxfam)</li>\n",
    "<li>Named entities are often the subject of sentiments so identifying them can be very useful</li>\n",
    "<li>Named entities may span more then one word (e.g., Columbia University)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597ff6f",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Finding named entities with spaCy</span>\n",
    "<li><span style=\"color:blue\">spaCy</span> is a NLP model trained on masses of data in various languages</li>\n",
    "<li>Good at named entity recognition</li>\n",
    "<li><a href=\"https://spacy.io/usage#quickstart\">Installation guide</a></li>\n",
    "<li>Install spacy and install the english language trained model en_core_web_md (3.7.0)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2135a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp39-cp39-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.22.4)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-macosx_10_9_x86_64.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.1-cp39-cp39-macosx_10_9_x86_64.whl (876 kB)\n",
      "\u001b[K     |████████████████████████████████| 876 kB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: setuptools in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-macosx_10_9_x86_64.whl (26 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.66.1)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (23.1)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-macosx_10_9_x86_64.whl (493 kB)\n",
      "\u001b[K     |████████████████████████████████| 493 kB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-macosx_10_9_x86_64.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 28.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: catalogue, srsly, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.3 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323df195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (2.4.2)\n",
      "Requirement already satisfied: spacy in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (3.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic) (0.5.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: jinja2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: setuptools in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U pydantic spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2670cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.0/en_core_web_md-3.7.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.8 MB 965 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-md==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: jinja2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.22.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (23.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (61.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.26.18)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/joshuagrajales/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47756ff0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Using spaCy</span>\n",
    "<li>Load the language model</li>\n",
    "<li>get the named entities</li>\n",
    "<li>spacy returns each entity, its location in the text, and the entity type (ORG, PERSON, DATE, MONEY, GPE, etc.)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ce39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68e0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "late afternoon 8 22 TIME\n",
      "a summer day 26 38 DATE\n",
      "New York 42 50 GPE\n",
      "twenty dollars 58 72 MONEY\n",
      "John Smith 167 177 PERSON\n",
      "Columbia University 200 219 ORG\n",
      "John Smith 311 321 PERSON\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "It was late afternoon on a summer day in New York. I had twenty dollars in my pocket.\n",
    "I was walking along thinking of many things. \n",
    "For e.g., I walked with my friend John Smith through the campus of Columbia University. I \n",
    "thought of birds, of bees, of sealing wax. I thought of cabbages and kings. My name is John Smith.\n",
    "\"\"\"\n",
    "doc = nlp(sample_text)\n",
    "for ent in doc.ents: #ent.text contains the ne; ent.label_ contains the type of ne\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17ea6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(late afternoon,\n",
       " a summer day,\n",
       " New York,\n",
       " twenty dollars,\n",
       " John Smith,\n",
       " Columbia University,\n",
       " John Smith)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68534643",
   "metadata": {},
   "source": [
    "<h3>Sentiment associated with entities in the news</h3>\n",
    "<li>We'll get the latest stories from www.slate.com</li>\n",
    "<li>Extract entities from these stories</li>\n",
    "<li>And then calculate the sentiment associated with the entities (in the stories) as the average sentiment of the sentences that contain the entity</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9426464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get stories from slate\n",
    "def get_slate_stories():\n",
    "    #followable_links contains the links to news and politics stories\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    url=\"https://www.slate.com\"\n",
    "    page = requests.get(url)\n",
    "    bs_page = BeautifulSoup(page.content,'lxml')\n",
    "    all_links = bs_page.find_all('a')\n",
    "    \n",
    "    #Define the list of categories that we want to follow\n",
    "    categories = ['news-and-politics']\n",
    "    \n",
    "    #followable_links will contain the title and the detail of each story\n",
    "    followable_links = list()\n",
    "    for link in all_links:\n",
    "        href = link.get('href') #get the link\n",
    "        if href: #If the link exists (sometimes it doesn't!)\n",
    "            for cat in categories:\n",
    "                if cat in href: #Only stories in the category\n",
    "                    title = link.get_text().strip() #Get the story title\n",
    "                    followable_links.append((title,href)) #Append (title, link) to followable links\n",
    "    \n",
    "    \n",
    "    #Iterate through followable links extracting the text of each story\n",
    "    #story_list is a list of the stories\n",
    "    #Note that some links will not contain an article_body section, those will be ignored (that's why the try except)\n",
    "    story_list = list()\n",
    "    count=0\n",
    "    for link in followable_links:\n",
    "        try:\n",
    "            page=BeautifulSoup(requests.get(link[1]).content,'lxml')\n",
    "            text=page.find('body').find('section',class_='article__body').get_text().strip()\n",
    "            story_list.append((link[0],text))\n",
    "            count+=1\n",
    "        except:\n",
    "            continue\n",
    "    return story_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0873c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_corpus = get_slate_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33a134c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(slate_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3946e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that gets named entities in a corpus\n",
    "def get_ne_from_corpus(corpus,types=[]): #If types is empty then all types, otherwise just the ones in the list\n",
    "    \n",
    "    #Set up spacy's ne analyzer\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    #Accumulate all the text in a single string (since we're looking across all articles)\n",
    "    all_text = \"\"\n",
    "    for doc in corpus:\n",
    "        all_text += doc[1]\n",
    "    doc = nlp(all_text)\n",
    "    text_ents = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in types or not types:\n",
    "            text_ents.add((ent.text,ent.label_))\n",
    "    return text_ents   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "860418f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Alina Habba', 'PERSON'),\n",
       " ('Allison Greenfield', 'PERSON'),\n",
       " ('Ambien', 'PERSON'),\n",
       " ('Andy Beshear', 'PERSON'),\n",
       " ('Antony Blinken', 'PERSON'),\n",
       " ('Arthur Engoron', 'PERSON'),\n",
       " ('Ashley Moody', 'PERSON'),\n",
       " ('Barack Obama', 'PERSON'),\n",
       " ('Barbara McQuade', 'PERSON'),\n",
       " ('Ben Mathis-Lilley', 'PERSON'),\n",
       " ('Benicio N. Frey', 'PERSON'),\n",
       " ('Benjamin Netanyahu', 'PERSON'),\n",
       " ('Bernie Sanders', 'PERSON'),\n",
       " ('Biden', 'PERSON'),\n",
       " ('Blinken', 'PERSON'),\n",
       " ('Brett Kavanaugh', 'PERSON'),\n",
       " ('Brian Sicknick', 'PERSON'),\n",
       " ('Busse', 'PERSON'),\n",
       " ('Carl Albert', 'PERSON'),\n",
       " ('Caroline Edwards', 'PERSON'),\n",
       " ('Chip Somodevilla', 'PERSON'),\n",
       " ('Chris Kise', 'PERSON'),\n",
       " ('Chuck Schumer', 'PERSON'),\n",
       " ('Chutkan', 'PERSON'),\n",
       " ('Cleng Peerson', 'PERSON'),\n",
       " ('Dan Kois', 'PERSON'),\n",
       " ('Dan McCaffery', 'PERSON'),\n",
       " ('Daniel Cameron', 'PERSON'),\n",
       " ('David Faris', 'PERSON'),\n",
       " ('David Oestreicher', 'PERSON'),\n",
       " ('Davis', 'PERSON'),\n",
       " ('DeSantis', 'PERSON'),\n",
       " ('Don Jr.', 'PERSON'),\n",
       " ('Donald Trump', 'PERSON'),\n",
       " ('E. H. Davis', 'PERSON'),\n",
       " ('Ed', 'PERSON'),\n",
       " ('Elizabeth Warren', 'PERSON'),\n",
       " ('Emerson F. Greenman', 'PERSON'),\n",
       " ('Emma Stone', 'PERSON'),\n",
       " ('Enrique Tarrio', 'PERSON'),\n",
       " ('Eric', 'PERSON'),\n",
       " ('Eric Trump', 'PERSON'),\n",
       " ('Explainer', 'PERSON'),\n",
       " ('Faris', 'PERSON'),\n",
       " ('Fred Kaplan', 'PERSON'),\n",
       " ('Frey', 'PERSON'),\n",
       " ('Futhark', 'PERSON'),\n",
       " ('GLOIEA', 'PERSON'),\n",
       " ('George Floyd’s', 'PERSON'),\n",
       " ('Glenn Youngkin', 'PERSON'),\n",
       " ('Greenfield', 'PERSON'),\n",
       " ('Gretchen Whitmer', 'PERSON'),\n",
       " ('Grinspoon', 'PERSON'),\n",
       " ('Haley', 'PERSON'),\n",
       " ('Harakat al-Muqawama al-Islamiya', 'PERSON'),\n",
       " ('Henry Kissinger’s', 'PERSON'),\n",
       " ('Hillary Clinton', 'PERSON'),\n",
       " ('Hjalmar R. Holand', 'PERSON'),\n",
       " ('Ivanka', 'PERSON'),\n",
       " ('Ivanka Trump', 'PERSON'),\n",
       " ('Jack Smith', 'PERSON'),\n",
       " ('James Pearce', 'PERSON'),\n",
       " ('James W. Clemens', 'PERSON'),\n",
       " ('Janet Protasiewicz', 'PERSON'),\n",
       " ('Jason W. Busse', 'PERSON'),\n",
       " ('Jenkins', 'PERSON'),\n",
       " ('Jerome Powell', 'PERSON'),\n",
       " ('Jesus', 'PERSON'),\n",
       " ('Jill Filipovic', 'PERSON'),\n",
       " ('Joe Biden', 'PERSON'),\n",
       " ('John Lauro', 'PERSON'),\n",
       " ('John McCain', 'PERSON'),\n",
       " ('John Roberts', 'PERSON'),\n",
       " ('Jonathan Mellis', 'PERSON'),\n",
       " ('Julian Khater', 'PERSON'),\n",
       " ('Ken Feder', 'PERSON'),\n",
       " ('Khater', 'PERSON'),\n",
       " ('Kim Reynolds’', 'PERSON'),\n",
       " ('Kleng Pedersen', 'PERSON'),\n",
       " ('Law', 'PERSON'),\n",
       " ('Leif Eriksson', 'PERSON'),\n",
       " ('Letitia James', 'PERSON'),\n",
       " ('Maria Stone', 'PERSON'),\n",
       " ('Mark Dudzik', 'PERSON'),\n",
       " ('Martin Scorsese', 'PERSON'),\n",
       " ('Martyn Whittock', 'PERSON'),\n",
       " ('Mary Harris', 'PERSON'),\n",
       " ('Mary Ziegler', 'PERSON'),\n",
       " ('McCain', 'PERSON'),\n",
       " ('McCarthyite', 'PERSON'),\n",
       " ('McQuade', 'PERSON'),\n",
       " ('Mellis', 'PERSON'),\n",
       " ('Michael Cohen', 'PERSON'),\n",
       " ('Mike Pence', 'PERSON'),\n",
       " ('Mitt Romney', 'PERSON'),\n",
       " ('Nathan Fielder', 'PERSON'),\n",
       " ('Nathaniel Rakich', 'PERSON'),\n",
       " ('Netanyahu', 'PERSON'),\n",
       " ('Nichols', 'PERSON'),\n",
       " ('Nikki', 'PERSON'),\n",
       " ('Nikki Haley', 'PERSON'),\n",
       " ('Nimarata Randhawa', 'PERSON'),\n",
       " ('Obama', 'PERSON'),\n",
       " ('Oestreicher', 'PERSON'),\n",
       " ('Osama bin Laden', 'PERSON'),\n",
       " ('Pearce', 'PERSON'),\n",
       " ('Pence', 'PERSON'),\n",
       " ('Peter Grinspoon', 'PERSON'),\n",
       " ('Poteau', 'PERSON'),\n",
       " ('Robert De Niro', 'PERSON'),\n",
       " ('Ron', 'PERSON'),\n",
       " ('Ron DeSantis', 'PERSON'),\n",
       " ('Ron DeSantisTweet\\n  \\n\\n\\n\\nShare\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\nComment',\n",
       "  'PERSON'),\n",
       " ('Ryan Nichols', 'PERSON'),\n",
       " ('Sam Adams', 'PERSON'),\n",
       " ('Sam Alito’s', 'PERSON'),\n",
       " ('Sarah Lipton-Lubet', 'PERSON'),\n",
       " ('Scott Nover', 'PERSON'),\n",
       " ('Sean Campbell', 'PERSON'),\n",
       " ('Shane Jenkins', 'PERSON'),\n",
       " ('Shawnee', 'PERSON'),\n",
       " ('Shirin Ali\\nKeeping Up', 'PERSON'),\n",
       " ('Sicknick', 'PERSON'),\n",
       " ('Tanya Chutkan', 'PERSON'),\n",
       " ('Trump', 'PERSON'),\n",
       " ('Trump Takes', 'PERSON'),\n",
       " ('Trumpism', 'PERSON'),\n",
       " ('Vikki', 'PERSON'),\n",
       " ('Vivek', 'PERSON'),\n",
       " ('Vivek Ramaswamy', 'PERSON'),\n",
       " ('W. Christopher Winter', 'PERSON'),\n",
       " ('William Chrestman', 'PERSON'),\n",
       " ('Willie Nelson', 'PERSON'),\n",
       " ('Winter', 'PERSON'),\n",
       " ('Yasmeen Khan', 'PERSON'),\n",
       " ('Youngkin', 'PERSON'),\n",
       " ('gnomedal', 'PERSON')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ne_from_corpus(slate_corpus,[\"PERSON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f4b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that gets sentiment (affect) for entities in a list of entites\n",
    "# in a corpus\n",
    "def get_ne_affect_from_corpus(corpus,entity_list):\n",
    "    import nltk\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    from nltk import sent_tokenize\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    #We need a running total of the sentiment and the total number of sentences\n",
    "    entity_dict=dict() \n",
    "    for entity in entity_list:\n",
    "        entity_dict[entity[0]] = [0,0] #[sentiment total, number of sentences]\n",
    "        \n",
    "    #Iterate through all the documents\n",
    "    for doc in corpus:\n",
    "        #tokenize one doc\n",
    "        sents = sent_tokenize(doc[1])\n",
    "        \n",
    "        #Iterate through all sentences\n",
    "        #For each entity, check if it is in the sentence\n",
    "        #If it is, add the \"compound\" vader score and a 1 for sentence count to the running total dict\n",
    "        #\n",
    "        for sent in sents:\n",
    "            for ent in entity_dict:\n",
    "                if ent in sent:\n",
    "                    va = analyzer.polarity_scores(sent)\n",
    "                    entity_dict[ent][0] += va['compound']\n",
    "                    entity_dict[ent][1] += 1\n",
    "    #Calculate net sentiment (total compound score/number of sentences) for each entity\n",
    "    sentiments = dict()\n",
    "    for ent,vals in entity_dict.items():\n",
    "        try:\n",
    "            sentiments[ent] = vals[0]/vals[1]\n",
    "        except:\n",
    "            sentiments[ent] = 0.0\n",
    "    return sentiments               \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1eeefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = get_ne_from_corpus(slate_corpus,[\"GPE\"])\n",
    "entity_sentiments = get_ne_affect_from_corpus(slate_corpus,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f2b3f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'New Jersey': 0.8779,\n",
       " 'Nova Scotia': 0.6697000000000001,\n",
       " 'Rhode Island': 0.6697000000000001,\n",
       " 'North Dakota': 0.6249,\n",
       " 'Florida': 0.5052,\n",
       " 'Houston': 0.4939,\n",
       " 'Iowa': 0.468675,\n",
       " 'Brooklyn': 0.44040000000000007,\n",
       " 'Montana': 0.4404,\n",
       " 'the United States': 0.42574615384615383,\n",
       " 'The United States': 0.4215,\n",
       " 'Newfoundland': 0.41786666666666666,\n",
       " 'Pennsylvania': 0.37195,\n",
       " 'Wisconsin': 0.36092499999999994,\n",
       " 'Illinois': 0.31245,\n",
       " 'Maine': 0.296,\n",
       " 'routeway': 0.296,\n",
       " 'Norway': 0.2839,\n",
       " 'Chicago': 0.2601,\n",
       " 'Iran': 0.2484,\n",
       " 'Poteau': 0.24695,\n",
       " 'Jurisprudence': 0.22935,\n",
       " 'Egypt': 0.21075,\n",
       " 'Gaza City': 0.2006,\n",
       " 'Columbus': 0.19999999999999998,\n",
       " 'Michigan': 0.19619999999999999,\n",
       " 'Gokstad': 0.19223333333333334,\n",
       " 'Winter': 0.1826,\n",
       " 'Iceland': 0.140225,\n",
       " 'America': 0.12470493827160488,\n",
       " 'Black Alabama': 0.1245,\n",
       " 'Texas': 0.104125,\n",
       " 'Kentucky': 0.0978,\n",
       " 'U.S.': 0.09696739130434785,\n",
       " 'Mexico': 0.0644,\n",
       " 'Canada': 0.0644,\n",
       " 'Minnesota': 0.05108461538461538,\n",
       " 'Manhattan': 0.04035,\n",
       " 'Kensington': 0.03134,\n",
       " 'Palestine': 0.030316666666666648,\n",
       " 'Lebanon': 0.0,\n",
       " 'the West Virginia State Museum': 0.0,\n",
       " 'Waukegan': 0.0,\n",
       " 'MO': 0.0,\n",
       " 'Saudi Arabia': 0.0,\n",
       " 'Washington Territory': 0.0,\n",
       " 'Greenland': 0.0,\n",
       " 'Spain': 0.0,\n",
       " 'Indiana': 0.0,\n",
       " 'Oregon': 0.0,\n",
       " 'Colorado': 0.0,\n",
       " 'Shawnee': 0.0,\n",
       " 'Icelander': 0.0,\n",
       " 'Jordan': 0.0,\n",
       " 'Oklahoma': -0.03541249999999999,\n",
       " 'India': -0.06526666666666667,\n",
       " 'Biden': -0.06788048780487806,\n",
       " 'Virginia': -0.11392272727272729,\n",
       " 'the Dakota Territory': -0.128,\n",
       " 'Trump': -0.1298128834355828,\n",
       " 'Washington': -0.163975,\n",
       " 'Heavener': -0.16582,\n",
       " 'Ohio': -0.1725578947368421,\n",
       " 'West Virginia': -0.18048333333333336,\n",
       " 'the Russian Empire': -0.296,\n",
       " 'Finland': -0.296,\n",
       " 'Israel': -0.3020431818181818,\n",
       " 'the West Bank': -0.30996666666666667,\n",
       " 'Queens': -0.3291,\n",
       " 'Salisbury': -0.37795,\n",
       " 'Ohioans': -0.3818,\n",
       " 'the West Virginia Archaeological Society': -0.3818,\n",
       " 'New York': -0.3848658536585366,\n",
       " 'D.C.': -0.47132000000000007,\n",
       " 'New York’s': -0.4767,\n",
       " 'Gaza': -0.48793333333333333,\n",
       " 'Gazan': -0.5027999999999999,\n",
       " 'Fairfax County': -0.5106,\n",
       " 'Qatar': -0.5154,\n",
       " 'PA': -0.5154,\n",
       " 'Georgia': -0.5267,\n",
       " 'Israel-Palestine': -0.5606,\n",
       " 'Moundsville': -0.58005,\n",
       " 'Tel Aviv': -0.5994,\n",
       " 'U.K.': -0.7559,\n",
       " 'Prussia': -0.8885,\n",
       " 'the German Empire': -0.8885,\n",
       " 'Denmark': -0.8885,\n",
       " 'the District of Columbia': -0.9099,\n",
       " 'Iraq': -0.9398}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(entity_sentiments.items(), key=lambda item: -1*item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "840e1897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Joe Biden': -0.03954285714285714, 'Donald Trump': -0.0160764705882353}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ne_affect_from_corpus(slate_corpus,[[\"Joe Biden\",'PERSON'],[\"Donald Trump\",'PERSON']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
