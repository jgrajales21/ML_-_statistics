{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Trees and Machine Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Decision trees are tree structures containing rules</li>\n",
    "<li>The leaf nodes of the tree are the \"learned\" categories (or threshold values)</li>\n",
    "<li>A path from the root to a leaf node represents a rule (or a decision path)</li>\n",
    "<li>Leaf nodes represent predictions (either a category or the mean (expected) value of a decision variable</li>\n",
    "<li>Each case filters through the tree from the root to a leaf node to get a prediction</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Why decision trees?</span>\n",
    "<li>Easy to understand </li>\n",
    "<li>Rule finding process is transparent</li>\n",
    "<li>Can handle \"mixed\" categorical(male/female) and numerical (age, number of siblings) data</li>\n",
    "<li>Can handle missing data </li>\n",
    "<li>Can be used to generate partial \"good\" solutions</li>\n",
    "<li>Can find non-linear patterns in the data</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Decision trees and non-linear patterns</span>\n",
    "<p></p>\n",
    "<img src=\"linear_model.png\">\n",
    "<img src=\"linear_classifier_accuracy.png\">\n",
    "<img src=\"non_linear_classifier.png\">\n",
    "<img src=\"decision_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Why not decision trees?</span>\n",
    "<li>Finding an optimal tree is a hard problem</li>\n",
    "<li>Overfitting is a problem</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example: Predicting wine quality</h1>\n",
    "<li><span style=\"color:blue\">Input features</span>: Chemical properties of wines</li>\n",
    "<li><span style=\"color:blue\">Wine quality</span>: A number between 0 and 10</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Import the data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "w_df = pd.read_csv(url,header=0,sep=';')\n",
    "w_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Examine the dependent variable</span>\n",
    "<li>Higher dv values indicate a better quality wine</li>\n",
    "<li>Lower dv values indicate a poorer quality wine</li>\n",
    "<li>We'll assume that the values are continuous</li>\n",
    "<li>And use the various features to predict wine quality</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7, 4, 8, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_df['quality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our quality, the variable we are trying to classify by, is an ordered set, we cannot use traditional classification trees, we need to use regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Build train and test samples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(w_df, test_size = 0.3)\n",
    "x_train_wine = train.iloc[0:,0:11]\n",
    "y_train_wine = train[['quality']]\n",
    "x_test_wine = test.iloc[0:,0:11]\n",
    "y_test_wine = test[['quality']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Choose a model</span>\n",
    "<p></p>\n",
    "<ul>\n",
    "<li><b>Classification trees</b>: Uses rules to classify cases into two or more categories (classify handwritten digits)</li>\n",
    "<ul>\n",
    "<li>Classification trees recursively split the data on a feature value</li>\n",
    "<li>Each split minimizes the entropy (also known as the impurity)</li>\n",
    "<li>Entropy is commonly measured using the GINI cost function (a measure of the probability of misclassification or 'purity')</li>\n",
    "    <li>Classifiers are used when the target variable is a set of unordered categories (handwritten digit recognition)</li>\n",
    "</ul>\n",
    "<li><b>Regression trees</b>: Uses rules to group data into target variable ranges (Wine Quality)</li>\n",
    "<ul>\n",
    "<li>Also split the data on feature values</li> \n",
    "    <li>Minimize cost (impurity). Usually the mean squared error</li>\n",
    "    <li>Regression trees are used when the target variable is continuous and ordered (wine quality from 0 to 10)</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Regression trees</span>\n",
    "<li>Regression trees use a greedy least squares algorithm to construct the tree</li>\n",
    "<li>The most common algorithm, roughly described below, is CART (Classification and Regression Trees) and this is what sklearn uses</li>\n",
    "<li>For each variable and each possible value the variable can take, calculate the mean square error generated by the split</li>\n",
    "<ul>\n",
    "<li>Since many variables are continuous, the set of possible values is pseudo continuous (the algorithm uses a finite subset of possible values, usually the set of actual values in the data)</li>\n",
    "</ul>\n",
    "<li>Calculate the Mean Square Error of each of the two halves for each split, and pick the split with the lowest MSE</li>\n",
    "<ul>\n",
    "    <li>The MSE is the square of the difference between each <i>actual</i> value in a half and the mean of all actual values in that half</li>\n",
    "    <li>The comined MSE is the sum of the two MSEs (each half of the split)</li>\n",
    "    <li> $ mse_{split} = \\Sigma_{i\\in S_{left}} (y_{i} - \\bar{y})^{2} + \\Sigma_{i\\in S_{right}} (y_{i} - \\bar{y})^{2} $ </li>\n",
    "    <li>Each (predictor,value) combination generates a split and the combination with the lowest MSE is chosen, generating two nodes, a left node and a right node</li>\n",
    "    <li>The process is then repeated at the left and right nodes until some stopping criterion is met</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Classification trees</span>\n",
    "<li>Classification trees use a greedy entropy minimization algorithm to construct the tree</li>\n",
    "<li><b>Entropy</b>: is a measure of uncertainty in the data<p></li>\n",
    "<ul>\n",
    "    <li>If there are c classes and the probability of drawing an item from class i is given by $ p_{i} $, then the entropy of the system is:\n",
    "        <ul>\n",
    "            <li> $E = \\sum_{i=1}^{c} -p_{i}\\times log_{2} p_{i} $ </li>\n",
    "        </ul>\n",
    "        <li>what is the uncertainty when you draw a marble from a box with 50 blue and 50 red marbles?</li>\n",
    "    <ul>\n",
    "        <li> $ p_{b} = 0.5 $</li>\n",
    "        <li> $ p_{r} = 0.5 $</li>\n",
    "        <li> $ E = (-0.5 \\times log_{2} 0.5) + (-0.5 \\times log_{2} 0.5) $</li>\n",
    "        <li> or, $ -0.5 \\times -1 + -0.5 * -1 = 1.0 $</li>\n",
    "    </ul>\n",
    "    <li>what is the uncertainty when you draw a marble from a box with 60 blue and 40 red marbles?</li>\n",
    "    <ul>\n",
    "        <li> $ p_{b} = 0.6 $</li>\n",
    "        <li> $ p_{r} = 0.4 $</li>\n",
    "        <li> $ E = -0.6 \\times log_{2} 0.6 + -0.4 \\times log_{2} 0.4 $</li>\n",
    "        <li> or, $ (-0.6 \\times -0.737) + (-0.4 \\times -1.322) = 0.971$</li>\n",
    "    </ul>\n",
    "    <li>what is the uncertainty in color when you draw a marble from a box of 100 blue marbles?</li>\n",
    "    <ul>\n",
    "        <li> $ p_{b} = 1.0 $</li>\n",
    "        <li> $ E = 1.0 \\times log_{2} 1.0 $</li>\n",
    "        <li> or, 0.0 </li>\n",
    "    </ul>\n",
    "    \n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Information gain vs Gini in classification trees</span>\n",
    "<p></p>\n",
    "<li>Classification trees use either information gain or Gini index in selecting splits</li>\n",
    "<li><b>Information gain</b> refers to the reduction in entropy resulting from a split. Classification trees use information gain as the decision factor when choosing a split</li>\n",
    "<ul> <li>$ e_{parent} - (w_{left} \\times e_{left} + w_{right} \\times e_{right}) $ where e is entropy and w is the weight given to a subnode</li>\n",
    "</ul>\n",
    "<li>The split with the maximum information gain is selected</li>\n",
    "<li><b>Gini index</b> aka <b>gini impurity</b> is an entropy measure that uses the probability that a selected data element is classified incorrectly</li>\n",
    "<ul>\n",
    "    <li> $ gini = \\sum_{i=1}^{c} p_{i}^{2} $ where c is the number of classes</li>\n",
    "    <li> $ giniimpurity = 1 - gini $</li>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Understanding Gini splits</span>\n",
    "<li>Example: predicting the grade of a student in a class</li>\n",
    "<li>One independet feature: attendance, a binary feature which takes two values A: Attend regularly or S: Skip often</li>\n",
    "<li>The following diagram represents the split for which we want to calculate impurity</li>\n",
    "<img src=\"gini.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> $ gini_{left} = (0.57)^{2} + (0.43)^{2} = 0.51 $ </li>\n",
    "<li> $ giniimpurity_{left} = 0.49 $ </li>\n",
    "<li> $ gini_{right} = (0.33)^{2} + (0.67)^{2} = 0.56 $ </li>\n",
    "<li> $ giniimpurity_{right} = 0.44 $</li>\n",
    "<li> $ w_{left} = \\frac{14}{20}$ </li>\n",
    "<li> $ w_{right} = \\frac{6}{20}$ </li>\n",
    "<li> $ gi_{split} = \\frac{14}{20} \\times 0.49 + \\frac{6}{20} \\times 0.44 = 0.475 $ </li>\n",
    "<li> CART tests all combinations of features and feature values and picks the split with the lowest impurity</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Tree depth: Stopping and Pruning Rules</span>\n",
    "<li>In the degenerate case, a decision tree algorithm can build a tree with exactly one training case in each leaf node</li>\n",
    "<li>This would be pointlessly overfitted!</li>\n",
    "<p></p>\n",
    "<span style=\"color:green;font-size:x-large\">controlling for overfitting</span>\n",
    "\n",
    "<li>Set a minimum count of observations in each leaf node. If the number of observations falls below the minimum, don't split the node any further</li>\n",
    "<li>Set a maximum tree <b>depth</b>. Once a path reaches a certain length, stop splitting that path</li>\n",
    "<li>Minimize <b>complexity cost</b>. Complexity cost in a decision tree is a function of the overall misclassification rate of the tree (we want the overall misclasification rate to be low) and the number of leaf nodes (we don't want too many categories because of the overfitting danger)</li>\n",
    "<li>Various other parameters (cf. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">The wine rating problem</span>\n",
    "<li>Since our dv is ordered and continuous, we'll choose a regression tree</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "wine_rgr = tree.DecisionTreeRegressor(max_depth=3,min_samples_leaf=20,min_samples_split=50)\n",
    "wine_rgr.fit(x_train_wine,y_train_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Evaluating a regressor</span>\n",
    "<li>Since a regressor is predicting continuous values of the dv, we can evaluate it like a linear regression model</li>\n",
    "<li><span style=\"color:blue\">R-Square</span> tells us how much of the variance in the data is explained by our model (how well the model fits the data)</li>\n",
    "<li><span style=\"color:blue\">mean square error</span> gives us an estimate of how far, on the average, are our predictions from actuals (this is better as a model comparison tool)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the R-Square for the predicted vs actuals on the text sample\n",
    "print(\"Training R-Square\",wine_rgr.score(x_train_wine,y_train_wine))\n",
    "print(\"Testing R-Square\",wine_rgr.score(x_test_wine,y_test_wine))\n",
    "#print(\"Training mean sq error\",wine_rgr.score(x_train_wine,y_train_wine))\n",
    "#print(\"Testing mean sq error\",wine_rgr.score(x_test_wine,y_test_wine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Visualizing the tree</span>\n",
    "<li>sklearn has a handy function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html\">plot_tree</a> for visualizing a tree</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#associate column names with features\n",
    "feature_names = w_df.columns[:-1]\n",
    "\n",
    "#set up a figure (mainly for the size)\n",
    "fig, ax = plt.subplots(figsize=(24, 28))\n",
    "\n",
    "#plot the tree\n",
    "tree.plot_tree(wine_rgr,feature_names=feature_names, max_depth=4, fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification trees</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:green;font-size:xx-large\">In-class problem: Classifying wine into good or bad wine</span>\n",
    "<li>assume any wine with quality less than 5.5 is a bad wine</li>\n",
    "<li>and any wine with quality greater than or equal to 5.5 is a good wine</li>\n",
    "<li>build a classifier that classifies the wine data into good and bad wines</li>\n",
    "<li>use the training data to build the classifier and report the accuracy <span style=\"color:red\">score</span> on the testing data</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build train and test data\n",
    "x_train_wine = train.iloc[0:,0:11]\n",
    "y_train_wine = train[['quality']]\n",
    "x_test_wine = test.iloc[0:,0:11]\n",
    "y_test_wine = test[['quality']]\n",
    "\n",
    "#Convert y values into categorical data\n",
    "y_train_wine_cat = y_train_wine >= 5.5\n",
    "y_test_wine_cat = y_test_wine >= 5.5\n",
    "\n",
    "#Set up and fit a model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "wine_clf = DecisionTreeClassifier(max_depth=4)\n",
    "#fit the data to the classifier\n",
    "wine_clf.fit(x_train_wine,y_train_wine_cat)\n",
    "\n",
    "#Report the accuracy score\n",
    "training_accuracy = wine_clf.score(x_train_wine,y_train_wine_cat)\n",
    "testing_accuracy = wine_clf.score(x_test_wine,y_test_wine_cat)\n",
    "print(training_accuracy)\n",
    "print(testing_accuracy)\n",
    "\n",
    "#Render the decision tree\n",
    "import matplotlib.pyplot as plt\n",
    "feature_names = w_df.columns[:-1]\n",
    "fig, ax = plt.subplots(figsize=(24, 28))\n",
    "tree.plot_tree(wine_clf,feature_names=feature_names, max_depth=4, fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
